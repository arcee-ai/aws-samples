{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Arcee Blitz on SageMaker through the Hugging Face hub\n",
    "\n",
    "[Arcee-Blitz (24B)](https://huggingface.co/arcee-ai/Arcee-Blitz) is a new Mistral-based 24B model distilled from DeepSeek, designed to be both fast and efficient. We view it as a practical “workhorse” model that can tackle a range of tasks without the overhead of larger architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install -qU sagemaker huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from IPython.display import Markdown, display\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.djl_inference.model import DJLModel\n",
    "from sagemaker_streaming import print_event_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "runtime_sm_client = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're deploying Arcee Blitz on a SageMaker real-time endpoint hosted on a GPU instance. If you need general information on real-time inference with Amazon SageMaker, please refer to the SageMaker [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html).\n",
    "\n",
    "The endpoint runs a DJLServing [Deep Learning Container](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-models-frameworks-djl-serving.html), powered by vLLM. vLLM enables high-performance text generation for the most popular open-source language models. \n",
    "\n",
    "The endpoint configuration focuses on cost efficiency. It uses a [g6e.12xlarge](https://aws.amazon.com/ec2/instance-types/g6e/) instance. This instance has four NVDIA L40S GPU, with 192GB of GPU RAM. Blitz has 24 billion 16-bit parameters, which can fit without the need for quantization.\n",
    "\n",
    "#### OpenAI compatibility\n",
    "\n",
    "The [OpenAI Messages API](https://huggingface.co/docs/text-generation-inference/messages_api) is available. This alllows you to invoke the endpoint in the same way you would invoke an OpenAI model. Likewise, the output format is identical to the OpenAI models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Define the endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"arcee-ai/Arcee-Blitz\"\n",
    "endpoint_name_prefix = \"Blitz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g6e endpoint\n",
    "\n",
    "real_time_inference_instance_type = \"ml.g6e.12xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DJLModel(model_id=model_id, role=role, env=None)\n",
    "\n",
    "# create a unique endpoint name\n",
    "timestamp = \"{:%Y-%m-%d-%H-%M-%S}\".format(datetime.datetime.now())\n",
    "endpoint_name = f\"{endpoint_name_prefix}-{timestamp}\"\n",
    "print(f\"Deploying endpoint {endpoint_name}\")\n",
    "\n",
    "# deploy the model\n",
    "response = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=real_time_inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_data_download_timeout=900,\n",
    "    container_startup_health_check_timeout=900,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint is in service, you will be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Define a test payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful AI assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Suggest 5 names for a new neighborhood pet food store. Names should be short, fun, easy to remember, and respectful of pets. \\\n",
    "        Explain why customers would like them.\",\n",
    "        },\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(model_sample_input),\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Visualize output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the raw JSON output in OpenAI format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the generated output with Markdown formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(output[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some more examples. Please feel free to tweak them and add your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Perform streaming inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Please write a friendly marketing pitch for a new SaaS agentic AI platform called Arcee Orchestra.\n",
    "We will send this pitch by email to business and technical decision-makers, so make it sound exciting yet professional.\n",
    "The contact email is sales@arcee.ai. Feel free to use emojis as appropriate.\n",
    "Arcee Orchestra makes it simple for enterprise users to create their own SLM-based agentic workflows,\n",
    "in order to automate a wide range of business processes.\"\"\"\n",
    "\n",
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly and helpful Marketing Manager working at Arcee.ai.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(model_sample_input),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "print_event_stream(response['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"As a friendly technical assistant engineer, answer the question in detail.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Why are transformers better models than LSTM? Explain step by step.\"},\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(model_sample_input),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "print_event_stream(response['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are Darlene, a friendly and helpful salesperson \\\n",
    "        working at Crystal River Classic Bikes, a classic motorcycle dealership in central Florida.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Using English, write a personalized customer email to get \\\n",
    "        them to sign up for a test ride on the new 2025 motorcycles that are visible at the dealership. \\\n",
    "        Tone should be warm and personal, make sure to weave in the customer information below. \\\n",
    "        Wyatt, your chief mechanic and road captain, has just won the 2024 State Award for Best Mechanic. \\\n",
    "        \\\n",
    "        Customer information:\\\n",
    "        - name: Julien \\\n",
    "        - last visit: 6 months ago for bike service \\\n",
    "        - Owns 2 bikes, a 2002 sporty bike and a 2007 cruiser \\\n",
    "        \",\n",
    "        },\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(model_sample_input),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "print_event_stream(response['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful AI coding assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"In the code below, analyse how Neon instructions are used for vectorization. Write a clear and compact summary.\n",
    "            \n",
    "static void ggml_gemv_q4_0_4x4_q8_0(int n, float * GGML_RESTRICT s, size_t bs, const void * GGML_RESTRICT vx, const void * GGML_RESTRICT vy, int nr, int nc) {\n",
    "    const int qk = QK8_0;\n",
    "    const int nb = n / qk;\n",
    "    const int ncols_interleaved = 4;\n",
    "    const int blocklen = 4;\n",
    "\n",
    "    assert (n % qk == 0);\n",
    "    assert (nc % ncols_interleaved == 0);\n",
    "\n",
    "    UNUSED(s);\n",
    "    UNUSED(bs);\n",
    "    UNUSED(vx);\n",
    "    UNUSED(vy);\n",
    "    UNUSED(nr);\n",
    "    UNUSED(nc);\n",
    "    UNUSED(nb);\n",
    "    UNUSED(ncols_interleaved);\n",
    "    UNUSED(blocklen);\n",
    "\n",
    "#if ! ((defined(_MSC_VER)) && ! defined(__clang__)) && defined(__aarch64__) && defined(__ARM_NEON) && defined(__ARM_FEATURE_DOTPROD)\n",
    "    if (ggml_cpu_has_neon() && ggml_cpu_has_dotprod()) {\n",
    "        const block_q4_0x4 * b_ptr = (const block_q4_0x4 *) vx;\n",
    "\n",
    "        for (int c = 0; c < nc; c += ncols_interleaved) {\n",
    "            const block_q8_0 * a_ptr = (const block_q8_0 *) vy;\n",
    "            float32x4_t acc = vdupq_n_f32(0);\n",
    "            for (int b = 0; b < nb; b++) {\n",
    "                int8x16_t b0 = vld1q_s8((const int8_t *) b_ptr->qs);\n",
    "                int8x16_t b1 = vld1q_s8((const int8_t *) b_ptr->qs + 16);\n",
    "                int8x16_t b2 = vld1q_s8((const int8_t *) b_ptr->qs + 32);\n",
    "                int8x16_t b3 = vld1q_s8((const int8_t *) b_ptr->qs + 48);\n",
    "                float16x4_t bd = vld1_f16((const __fp16 *) b_ptr->d);\n",
    "\n",
    "                int8x16_t a0 = vld1q_s8(a_ptr->qs);\n",
    "                int8x16_t a1 = vld1q_s8(a_ptr->qs + qk/2);\n",
    "                float16x4_t ad = vld1_dup_f16((const __fp16 *) &a_ptr->d);\n",
    "\n",
    "                int32x4_t ret = vdupq_n_s32(0);\n",
    "\n",
    "                ret = vdotq_laneq_s32(ret, b0 << 4, a0, 0);\n",
    "                ret = vdotq_laneq_s32(ret, b1 << 4, a0, 1);\n",
    "                ret = vdotq_laneq_s32(ret, b2 << 4, a0, 2);\n",
    "                ret = vdotq_laneq_s32(ret, b3 << 4, a0, 3);\n",
    "\n",
    "                ret = vdotq_laneq_s32(ret, b0 & 0xf0U, a1, 0);\n",
    "                ret = vdotq_laneq_s32(ret, b1 & 0xf0U, a1, 1);\n",
    "                ret = vdotq_laneq_s32(ret, b2 & 0xf0U, a1, 2);\n",
    "                ret = vdotq_laneq_s32(ret, b3 & 0xf0U, a1, 3);\n",
    "\n",
    "                acc = vfmaq_f32(acc, vcvtq_n_f32_s32(ret, 4),\n",
    "                                vmulq_f32(vcvt_f32_f16(ad), vcvt_f32_f16(bd)));\n",
    "                a_ptr++;\n",
    "                b_ptr++;\n",
    "            }\n",
    "            vst1q_f32(s, acc);\n",
    "            s += ncols_interleaved;\n",
    "        }\n",
    "        return;\n",
    "    }\n",
    "#endif // #if ! ((defined(_MSC_VER)) && ! defined(__clang__)) && defined(__aarch64__) && defined(__ARM_NEON) && defined(__ARM_FEATURE_DOTPROD)\n",
    "    float sumf[4];\n",
    "    int sumi;\n",
    "\n",
    "    const block_q8_0 * a_ptr = (const block_q8_0 *) vy;\n",
    "    for (int x = 0; x < nc / ncols_interleaved; x++) {\n",
    "        const block_q4_0x4 * b_ptr = (const block_q4_0x4 *) vx + (x * nb);\n",
    "\n",
    "        for (int j = 0; j < ncols_interleaved; j++) sumf[j] = 0.0;\n",
    "        for (int l = 0; l < nb; l++) {\n",
    "            for (int k = 0; k < (qk / (2 * blocklen)); k++) {\n",
    "                for (int j = 0; j < ncols_interleaved; j++) {\n",
    "                    sumi = 0;\n",
    "                    for (int i = 0; i < blocklen; ++i) {\n",
    "                        const int v0 = (int8_t) (b_ptr[l].qs[k * ncols_interleaved * blocklen + j * blocklen + i] << 4);\n",
    "                        const int v1 = (int8_t) (b_ptr[l].qs[k * ncols_interleaved * blocklen + j * blocklen + i] & 0xF0);\n",
    "                        sumi += ((v0 * a_ptr[l].qs[k * blocklen + i]) + (v1 * a_ptr[l].qs[k * blocklen + i + qk / 2])) >> 4;\n",
    "                    }\n",
    "                    sumf[j] += sumi * GGML_FP16_TO_FP32(b_ptr[l].d[j]) * GGML_FP16_TO_FP32(a_ptr[l].d);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        for (int j = 0; j < ncols_interleaved; j++) s[x * ncols_interleaved + j] = sumf[j];\n",
    "    }\n",
    "}\n",
    "\n",
    "    \"\"\",\n",
    "        },\n",
    "    ],\n",
    "    \"max_tokens\": 8192,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(model_sample_input),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "print_event_stream(response['Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up\n",
    "\n",
    "Please don't forget to run the cells below to delete all resources and avoid unecessary charges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(endpoint_name)\n",
    "model.sagemaker_session.delete_endpoint_config(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for trying out Arcee Blitz on SageMaker. We have only scratched the surface of what you can do with this model.\n",
    "\n",
    "We'd be happy to hear from you, learn more about your use case, and help you build your next AI-driven solution. Please reach out to julien@arcee.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

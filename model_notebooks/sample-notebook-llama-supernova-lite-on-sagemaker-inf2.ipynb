{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Llama SuperNova Lite on SageMaker and Inferentia2\n",
    "\n",
    "This sample notebook shows you how to deploy [Llama SuperNova Lite](https://huggingface.co/arcee-ai/Llama-3.1-SuperNova-Lite) on Inferentia2 using Amazon SageMaker. Llama SuperNova Lite is a conversational model developed by [Arcee.ai](https://www.arcee.ai).\n",
    "\n",
    "Llama-3.1-SuperNova-Lite is an 8B parameter model developed by Arcee.ai, based on the Llama-3.1-8B-Instruct architecture. It is a distilled version of the larger Llama-3.1-405B-Instruct model, leveraging offline logits extracted from the 405B parameter variant. This 8B variation of Llama-3.1-SuperNova maintains high performance while offering exceptional instruction-following capabilities and domain-specific adaptability.\n",
    "\n",
    "The model was trained using a state-of-the-art distillation pipeline and an instruction dataset generated with EvolKit, ensuring accuracy and efficiency across a wide range of tasks. For more information on its training, visit blog.arcee.ai.\n",
    "\n",
    "## Use cases\n",
    "\n",
    "Llama-3.1-SuperNova-Lite excels in both benchmark performance and real-world applications, providing the power of large-scale models in a more compact, efficient form ideal for organizations seeking high performance with reduced resource requirements.\n",
    "\n",
    "## Pre-requisites\n",
    "1. Before running this notebook, please make sure you got this notebook from the model catalog on SageMaker AWS Management Console.\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**.\n",
    "\n",
    "## Contents\n",
    "1. [Import dependencies](#1.-Import-dependencies)\n",
    "\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "    1. [Define the endpoint configuration](#A.-Define-the-endpoint-configuration)\n",
    "    2. [Create the endpoint](#B.-Create-the-endpoint)\n",
    "    3. [Define a test payload](#C.-Define-a-test-payload)\n",
    "    4. [Perform real-time inference](#D.-Perform-real-time-inference)\n",
    "    5. [Visualize output](#E.-Visualize-output)\n",
    "    6. [Perform streaming inference](#F.-Perform-streaming-inference)\n",
    "\n",
    "\n",
    "3. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    2. [Delete the endpoint](#B.-Delete-the-endpoint)\n",
    "    \n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install -qU boto3 sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from IPython.display import Markdown, display\n",
    "from sagemaker import Model, get_execution_role, image_uris\n",
    "from sagemaker.djl_inference.model import DJLModel\n",
    "from sagemaker_streaming import print_event_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_bucket = sagemaker_session.default_bucket()\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're deploying Llama SuperNova Lite on a SageMaker real-time endpoint. If you need general information on real-time inference with Amazon SageMaker, please refer to the SageMaker [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html).\n",
    "\n",
    "The endpoint runs a Large Model Inference (LMI) [Deep Learning Container](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/index.html), powered by the [DJLServing](https://docs.djl.ai/master/docs/serving/index.html) server and the [transformers-neuronx](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/user_guides/tnx_user_guide.html) library.\n",
    "\n",
    "We will deploy the model to an [inf2.2xlarge](https://aws.amazon.com/ec2/instance-types/inf2/) instance. This instance has two NeuronCores v2, with a total of 32 GB of accelerator RAM.\n",
    "\n",
    "For flexibility, you can pick from two sample configurations. Please make sure to run just one of the configuration in the cells below.\n",
    "\n",
    "1. Download the model from the Hugging Face hub and compile it on the fly\n",
    "\n",
    "    This configuration is more flexible, as we can pick the batch size and the sequence length at deployment time.\n",
    "\n",
    "    However, the endpoint creation time is longer, as we need to download the model from the hub and compile it. In this example, endpoint creation should take 12-13 minutes, including about 4 minutes of model compilation.\n",
    "   \n",
    "    Accessing the hub may also not be possible in air-gapped deployment scenarios. We could load a Hugging Face model previously saved in S3, which would remove the dependency on the hub and speed up download a bit. Or course, model compilation would still be required.\n",
    "\n",
    "3. Load a precompiled model from an Amazon S3 bucket (batch size 4, sequence length 4096)\n",
    "\n",
    "    You should follow these [instructions](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/tutorials/tnx_aot_tutorial.html) to pre-compile and package the model. \n",
    "\n",
    "    Deployment is faster as we skip compilation: in this example, endpoint creation should take about 7-8 minutes.\n",
    "\n",
    "    Also, a compiled model allows you to lock down the configuration of your endpoint and make sure it's deployed with static settings\n",
    "\n",
    "    However, batch size and sequence length are fixed: you'll need several compiled models for different settings.\n",
    "\n",
    "#### OpenAI compatibility\n",
    "\n",
    "For both configurations, the endpoint supports the [OpenAI Messages API](https://huggingface.co/docs/text-generation-inference/messages_api). This allows you to invoke the endpoint in the same way you would invoke an OpenAI model. Likewise, the output format will be identical to the OpenAI models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Define the endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"arcee-ai/Llama-3.1-SuperNova-Lite\"\n",
    "model_name_prefix = \"llama-supernova-lite-neuron\"\n",
    "instance_type = \"ml.inf2.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-neuronx\",\n",
    "    region=sagemaker_session.boto_session.region_name,\n",
    "    version=\"0.29.0\",\n",
    ")\n",
    "\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First configuration: deploy with a model compiled on the fly\n",
    "\n",
    "First, we define serving parameters in the model environment. Then, we create the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_environment = {\n",
    "    \"OPTION_ENTRYPOINT\": \"djl_python.transformers_neuronx\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"auto\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"2\",\n",
    "    #  1 doesn't work, use 2-16 https://github.com/deepjavalibrary/djl-serving/issues/2354\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"2\",\n",
    "    \"OPTION_N_POSITIONS\": \"8192\",\n",
    "    \"OPTION_MODEL_LOADING_TIMEOUT\": \"900\",\n",
    "}\n",
    "\n",
    "djl_model = DJLModel(\n",
    "    model_id=model_id, image_uri=image_uri, env=model_environment, role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done this, we can [create the endpoint](#B.-Create-the-endpoint)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second configuration: deploy with a precompiled model (batch size 4, sequence length 4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've followed the [instructions](https://docs.djl.ai/master/docs/serving/serving/docs/lmi/tutorials/tnx_aot_tutorial.html), the model and its compiled version are available in S3, in this example at `s3://arcee-uswest2-marketplace-models/XXX/`\n",
    "\n",
    "```\n",
    "|- config.json\n",
    "|- generation_config.json (not in the instructions, fixes a warning at loading time)\n",
    "|- special_tokens_map.json\n",
    "|- tokenizer*.*\n",
    "|- checkpoint/\n",
    "|- - config.json\n",
    "|- - generation_config.json\n",
    "|- - model*.safetensors\n",
    "|- - model.safetensors.index.json\n",
    "|- compiled/\n",
    "|- - VERSION\n",
    "|- - *.neff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define serving parameters in a configuration file. Please make sure that `tensor_parallel_degree`, `n_positions` and `max_rolling_batch_size` match the values used at compilation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python\n",
    "option.entryPoint=djl_python.transformers_neuronx\n",
    "option.model_id=s3://arcee-uswest2-marketplace-models/XXX/\n",
    "option.tensor_parallel_degree=2\n",
    "option.n_positions=4096\n",
    "option.rolling_batch=auto\n",
    "option.max_rolling_batch_size=4\n",
    "option.model_loading_timeout=3600\n",
    "option.enable_mixed_precision_accumulation=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we package the configuration file and upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_artifact = sagemaker_session.upload_data(\n",
    "    \"mymodel.tar.gz\", sagemaker_bucket, model_name_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the model. Once we've done this, we can [create the endpoint](#B.-Create-the-endpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djl_model = Model(model_data=properties_artifact, image_uri=image_uri, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# create a unique endpoint name\n",
    "timestamp = \"{:%Y-%m-%d-%H-%M-%S}\".format(datetime.datetime.now())\n",
    "endpoint_name = f\"{model_name_prefix}-{timestamp}\"\n",
    "print(f\"Deploying endpoint {endpoint_name}\")\n",
    "\n",
    "# deploy the model\n",
    "predictor = djl_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_data_download_timeout=900,\n",
    "    container_startup_health_check_timeout=900,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint is in service, you will be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Define a test payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful AI assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Suggest 5 names for a new neighborhood pet food store. Names should be short, fun, easy to remember, and respectful of pets. \\\n",
    "        Explain why customers would like them.\",\n",
    "        },\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(model_sample_input),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Visualize output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the raw JSON output in OpenAI format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the generated output with Markdown formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(output[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some more examples. Please feel free to tweak them and add your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Please write a friendly marketing pitch for a new SaaS AI platform called Arcee Cloud.\n",
    "We will send this pitch by email to business and technical decision-makers, so make it sound exciting yet professional.\n",
    "The contact email is sales@arcee.ai. Feel free to use emojis as appopriate.\n",
    "Arcee Cloud makes it simple for enterprise users to tailor open-source small language models to their own domain knowledge,\n",
    "in order to build high-quality, cost-effective and secure AI solutions.\"\"\"\n",
    "\n",
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly and helpful Marketing Manager working at Arcee.ai.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(model_sample_input),\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "display(Markdown(output[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Perform streaming inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"As a friendly technical assistant engineer, answer the question in detail.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Why are transformers better models than LSTM?\"},\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=predictor.endpoint_name,\n",
    "    Body=json.dumps(model_sample_input),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "print_event_stream(response['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are Darlene, a friendly and helpful salesperson \\\n",
    "        working at Crystal River Classic Bikes, a classic motorcycle dealership in central Florida.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Using English, write a personalized customer email to get \\\n",
    "        them to sign up for a test ride on the new 2025 motorcycles that are visible at the dealership. \\\n",
    "        Tone should be warm and personal, make sure to weave in the customer information below. \\\n",
    "        Wyatt, your chief mechanic and road captain, has just won the 2024 State Award for Best Mechanic. \\\n",
    "        \\\n",
    "        Customer information:\\\n",
    "        - name: Julien \\\n",
    "        - last visit: 6 months ago for bike service \\\n",
    "        - Owns 2 bikes, a 2002 sporty bike and a 2007 cruiser \\\n",
    "        \",\n",
    "        },\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=predictor.endpoint_name,\n",
    "    Body=json.dumps(model_sample_input),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "print_event_stream(response['Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up\n",
    "\n",
    "Please don't forget to run the cells below to delete all resources and avoid unecessary charges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djl_model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for trying out Llama-Spark on Inferentia2 and SageMaker. We have only scratched the surface of what you can do with this model.\n",
    "\n",
    "We'd be happy to hear from you, learn more about your use case, and help you build your next AI-driven solution. Please reach out to julien@arcee.ai."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

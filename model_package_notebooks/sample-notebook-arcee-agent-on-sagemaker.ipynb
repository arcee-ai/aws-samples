{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Arcee-Agent on SageMaker through Model Packages\n",
    "\n",
    "This sample notebook shows you how to deploy [Arcee Agent](https://huggingface.co/arcee-ai/Arcee-Agent) using Amazon SageMaker. Arcee Agent is a cutting-edge 7B parameter language model developed by [Arcee.ai](https://www.arcee.ai) specifically for function calling and tool use. Here, we'll build an agent app that invokes the Yahoo Finance API to retrive company information.\n",
    "\n",
    "Arcee Agent excels at interpreting, executing, and chaining function calls. This capability allows it to interact seamlessly with a wide range of external tools, APIs, and services. The model is compatible with various tool use formats, including Glaive FC v2, Salesforce, and Agent-FLAN. Arcee-Agent performs best when using the VLLM OpenAI FC format, but it also excels with prompt-based solutions.\n",
    "\n",
    "Initialized from Qwen2-7B, it rivals the performance of much larger models while maintaining efficiency and speed. This model is particularly suited for developers, researchers, and businesses looking to implement sophisticated AI-driven solutions without the computational overhead of larger language models.\n",
    "\n",
    "## Use cases\n",
    "Arcee Agent's unique capabilities make it an invaluable asset for businesses across various industries. Here are some specific use cases:\n",
    "\n",
    "* Customer Support Automation:\n",
    "    Implement AI-driven chatbots that handle complex customer inquiries and support tickets.\n",
    "    Automate routine support tasks such as password resets, order tracking, and FAQ responses.\n",
    "    Integrate with CRM systems to provide personalized customer interactions based on user history.\n",
    "\n",
    "* Sales and Marketing Automation:\n",
    "    Automate lead qualification and follow-up using personalized outreach based on user behavior.\n",
    "    Generate dynamic marketing content tailored to specific audiences and platforms.\n",
    "    Analyze customer feedback from various sources to inform marketing strategies.\n",
    "\n",
    "* Operational Efficiency:\n",
    "    Automate administrative tasks such as scheduling, data entry, and report generation.\n",
    "    Implement intelligent assistants for real-time data retrieval and analysis from internal databases.\n",
    "    Streamline project management with automated task assignment and progress tracking.\n",
    "\n",
    "* Financial Services Automation:\n",
    "    Automate financial reporting and compliance checks.\n",
    "    Implement AI-driven financial advisors for personalized investment recommendations.\n",
    "    Integrate with financial APIs to provide real-time market analysis and alerts.\n",
    "\n",
    "* Healthcare Solutions:\n",
    "    Automate patient record management and data retrieval for healthcare providers.\n",
    "\n",
    "* E-commerce Enhancements:\n",
    "    Create intelligent product recommendation systems based on user preferences and behavior.\n",
    "    Automate inventory management and supply chain logistics.\n",
    "    Implement AI-driven pricing strategies and promotional campaigns.\n",
    "\n",
    "* Human Resources Automation:\n",
    "    Automate candidate screening and ranking based on resume analysis and job requirements.\n",
    "    Implement virtual onboarding assistants to guide new employees through the onboarding process.\n",
    "    Analyze employee feedback and sentiment to inform HR policies and practices.\n",
    "\n",
    "* Legal Services Automation:\n",
    "    Automate contract analysis and extraction of key legal terms and conditions.\n",
    "    Implement AI-driven tools for legal research and case law summarization.\n",
    "    Develop virtual legal assistants to provide preliminary legal advice and document drafting.\n",
    "\n",
    "* Educational Tools:\n",
    "    Create personalized learning plans and content recommendations for students.\n",
    "    Automate grading and feedback for assignments and assessments.\n",
    "\n",
    "* Manufacturing and Supply Chain Automation:\n",
    "    Optimize production schedules and inventory levels using real-time data analysis.\n",
    "    Implement predictive maintenance for machinery and equipment.\n",
    "    Automate quality control processes through data-driven insights.\n",
    "\n",
    "## Pre-requisites\n",
    "1. Before running this notebook, please make sure you got this notebook from the model catalog on SageMaker AWS Management Console.\n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**.\n",
    "\n",
    "## Contents\n",
    "1. [Select model package](#1.-Select-model-package)\n",
    "\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "    1. [Define the endpoint configuration](#A.-Define-the-endpoint-configuration)\n",
    "    2. [Create the endpoint](#B.-Create-the-endpoint)\n",
    "    3. [Define a test payload](#C.-Define-a-test-payload)\n",
    "    4. [Perform real-time inference](#D.-Perform-real-time-inference)\n",
    "    5. [Visualize output](#E.-Visualize-output)\n",
    "\n",
    "3. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    2. [Delete the endpoint](#B.-Delete-the-endpoint)\n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install -q yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Select the model package\n",
    "Confirm that you received this notebook from model catalog on SageMaker AWS Management Console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for Model Packages (initially only us-east-1 and eu-west-1 is supported)\n",
    "model_package_map = {\n",
    "    \"us-east-1\": \"<Model Partner to specify Model package ARN corresponding to their AWS region>\",\n",
    "    \"us-west-2\": \"arn:aws:sagemaker:us-west-2:014498647618:model-package/Arcee-Agent-tgi-test\",\n",
    "    \"eu-west-1\": \"<Model Partner to specify Model package ARN corresponding to their AWS region>\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from IPython.display import Markdown, display\n",
    "from sagemaker import ModelPackage, get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in model_package_map.keys():\n",
    "    raise (\"UNSUPPORTED REGION\")\n",
    "\n",
    "model_package_arn = model_package_map[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "runtime_sm_client = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're deploying Arcee-Agent on a SageMaker real-time endpoint hosted on a GPU instance. If you need general information on real-time inference with Amazon SageMaker, please refer to the SageMaker [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html).\n",
    "\n",
    "The endpoint runs a Hugging Face [Deep Learning Container](https://huggingface.co/docs/sagemaker/index), powered by the Hugging Face [Text Generation Inference](https://huggingface.co/docs/text-generation-inference/index) Server (TGI). TGI enables high-performance text generation for the most popular open-source language models. \n",
    "\n",
    "For flexibility, you can pick from two sample configurations, depending your use case and the instances types available to you. Please make sure to run just one of the configuration cells below.\n",
    "\n",
    "The endpoint configuration focuses on cost efficiency. It uses a [g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/) instance. This instance has a single NVDIA A10G GPU, with 24 GB of GPU RAM. Arcee-Agent has 7 billion 16-bit parameters, which can easily fit without the need for quantization.\n",
    "\n",
    "For context size, we use the default value defined by the TGI inference server, i.e. 4 KB.\n",
    "\n",
    "We enable the [OpenAI Messages API](https://huggingface.co/docs/text-generation-inference/messages_api) available in TGI. This will alllow you to invoke the endpoint in the same way you would invoke an OpenAI model. Likewise, the output format will be identical to the OpenAI models. If that's not desirable, you can simply comment out the line setting `MESSAGES_API_ENABLED` to `true`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Define the endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Arcee-Agent\"\n",
    "real_time_inference_instance_type = \"ml.g5.2xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployable model from the model package.\n",
    "model = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=model_package_arn,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# create a unique endpoint name\n",
    "timestamp = \"{:%Y-%m-%d-%H-%M-%S}\".format(datetime.datetime.now())\n",
    "endpoint_name = f\"{model_name}-{timestamp}\"\n",
    "print(f\"Deploying endpoint {endpoint_name}\")\n",
    "\n",
    "# deploy the model\n",
    "response = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=real_time_inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_data_download_timeout=3600,\n",
    "    container_startup_health_check_timeout=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint is in service, you will be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Define a test payload\n",
    "\n",
    "Let's define a function to invoke the endpoint and run a sample query to check that the model works as it should. We're not using its agent behavior, just answering a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_endpoint(system_prompt, user_prompt, max_tokens=128):\n",
    "    model_sample_input = {\n",
    "        \"model\": \"tgi\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(model_sample_input),\n",
    "    )\n",
    "    output = json.loads(response[\"Body\"].read())\n",
    "    return output[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = invoke_endpoint(\n",
    "    \"As a friendly technical assistant engineer, answer the question in detail.\",\n",
    "    \"Why are transformers better models than LSTM?\",\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Define the agent behavior and its functions\n",
    "\n",
    "Now, let's define the system prompt. Its purpose is to tell the model which functions are available and when it should call them. We list four functions to get the latest stock price, the CEO name, a company summary, and a general-purpose function to answer other types of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt = \"\"\"You have four primary functions: checking the last price of a specified stock, finding the name of a company CEO, finding what a company does, and answering specific questions about a company. Use the appropriate function based on the user's query.\n",
    "\n",
    "### Functions:\n",
    "\n",
    "1. **get_stock_price(company_name: str, stock_symbol: str) -> str**\n",
    "- This function returns the last close price of a specified stock.\n",
    "- Input: A company name (e.g., Mc Donalds), which you must convert to a stock symbol (e.g., MCD).\n",
    "- Output: A string containing the last close price of the specified stock (e.g., \"The last closing price of Mc Donalds (MCD) is $250.00\").\n",
    "\n",
    "2. **get_ceo_name(company_name: str, stock_symbol: str) -> str**\n",
    "- This function returns the name of the CEO of a specified company.\n",
    "- Input: A company name (e.g., Mc Donalds), which you must convert to a stock symbol (e.g., MCD).\n",
    "- Output: A string containing the name of the CEO of the specified company (e.g., \"The CEO of Mc Donalds is John Doe\").\n",
    "\n",
    "3. **get_company_summary(company_name: str, stock_symbol: str) -> str**\n",
    "- This function returns a summary describing the business activities of a specified company.\n",
    "- Input: A company name (e.g., Mc Donalds), which you must convert to a stock symbol (e.g., MCD).\n",
    "- Output: A string containing a detailed summary of the specified company's business activities.\n",
    "\n",
    "4. **answer_general_question(question: str) -> str**\n",
    "- This function answers questions in general.\n",
    "- Input: a user question.\n",
    "- Output: your best answer to the user question.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "- If the user asks a question related to the price of a stock, use the `get_stock_price` function.\n",
    "- If the user asks a question related to the CEO of a company, use the `get_ceo_name` function.\n",
    "- If the user asks a general question about a company's activities, use the `get_company_summary` function.\n",
    "- If the user asks any other question, use the `answer_general_question` function. Only return the result of the function call, not your internal reasoning.\n",
    "- Only return the result of the function call.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write the code for these functions. The first three use the Yahoo Finance [`yfinance`](https://pypi.org/project/yfinance) package, the last one simply invokes the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_price(company_name, stock_symbol):\n",
    "    stock = yfinance.Ticker(stock_symbol)\n",
    "    price = stock.history(period=\"1d\")[\"Close\"].values[0]\n",
    "    return (\n",
    "        f\"The last closing price of {company_name} ({stock_symbol}) was ${price:.2f}.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_ceo_name(company_name, stock_symbol):\n",
    "    stock = yfinance.Ticker(stock_symbol)\n",
    "    info = stock.info\n",
    "    ceo = info[\"companyOfficers\"][0][\"name\"]\n",
    "    return f\"The CEO of {company_name} is {ceo}. The full job title is {info['companyOfficers'][0]['title']}.\"\n",
    "\n",
    "\n",
    "def get_company_summary(company_name, stock_symbol):\n",
    "    stock = yfinance.Ticker(stock_symbol)\n",
    "    summary = stock.info[\"longBusinessSummary\"]\n",
    "    return (\n",
    "        f\"{company_name} ({stock_symbol}) is a company that is involved in {summary}.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def answer_general_question(question):\n",
    "    return invoke_endpoint(\"You are a helpful AI assistant\", question, max_tokens=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a function invoking the agent behavior in Arcee-Agent. The response is the Python function call to perform, which we print. Then, we extract it from the model response, run it, and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(user_prompt):\n",
    "    try:\n",
    "        func = invoke_endpoint(agent_prompt, user_prompt).strip()\n",
    "        print(f\"Running this function call: {func}\")\n",
    "        code = f\"result = {func}\"\n",
    "        local_vars = {}\n",
    "        exec(code, globals(), local_vars)\n",
    "        ans = local_vars.get(\"result\").strip()\n",
    "        return ans\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Run functions\n",
    "\n",
    "Now, let's run some examples. Feel free to try the prompts below and use your own. You should be able to get information on any listed company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(user_prompt):\n",
    "    response = run_agent(user_prompt)\n",
    "    display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Who is the CEO of Tesla?\"\n",
    "ask_question(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"What the stock price for Deutsche Telekom?\"\n",
    "ask_question(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"What does Cummins build?\"\n",
    "ask_question(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"Who are the main competitors of Hilton?\"\n",
    "ask_question(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up\n",
    "\n",
    "Please don't forget to run the cells below to delete all resources and avoid unecessary charges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(endpoint_name)\n",
    "model.sagemaker_session.delete_endpoint_config(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for trying out Arcee-Agent on SageMaker. We have only scratched the surface of what you can do with this model.\n",
    "\n",
    "We'd be happy to hear from you, learn more about your use case, and help you build your next AI-driven solution. Please reach out to julien@arcee.ai."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

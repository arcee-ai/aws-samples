{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SuperNova-Medius on SageMaker through Model Packages\n",
    "\n",
    "This sample notebook shows you how to deploy [SuperNova-Medius](https://huggingface.co/arcee-ai/SuperNova-Medius) using Amazon SageMaker. SuperNova-Medius is a conversational model developed by [Arcee.ai](https://www.arcee.ai). \n",
    "\n",
    "Arcee-SuperNova-Medius is a 14B parameter language model developed by Arcee.ai, built on the Qwen2.5-14B-Instruct architecture. This unique model is the result of a cross-architecture distillation pipeline, combining knowledge from both the Qwen2.5-72B-Instruct model and the Llama-3.1-405B-Instruct model. By leveraging the strengths of these two distinct architectures, SuperNova-Medius achieves high-quality instruction-following and complex reasoning capabilities in a mid-sized, resource-efficient form.\n",
    "\n",
    "## Use cases\n",
    "\n",
    "SuperNova-Medius is designed to excel in a variety of business use cases, including customer support, content creation, and technical assistance, while maintaining compatibility with smaller hardware configurations. It’s an ideal solution for organizations looking for advanced capabilities without the high resource requirements of larger models like our SuperNova-70B.\n",
    "\n",
    "## Pre-requisites\n",
    "1. This notebook works for models listed in SageMaker JumpStart and on AWS Marketplace. If you'd like to deploy from AWS Marketplace, please make sure you have previously subscribed to the model. \n",
    "1. **Note**: This notebook contains elements which render correctly in Jupyter interface. Open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "1. Ensure that IAM role used has **AmazonSageMakerFullAccess**.\n",
    "\n",
    "## Contents\n",
    "1. [Select model package](#1.-Select-model-package)\n",
    "\n",
    "2. [Create an endpoint and perform real-time inference](#2.-Create-an-endpoint-and-perform-real-time-inference)\n",
    "    1. [Define the endpoint configuration](#A.-Define-the-endpoint-configuration)\n",
    "    2. [Create the endpoint](#B.-Create-the-endpoint)\n",
    "    3. [Define a test payload](#C.-Define-a-test-payload)\n",
    "    4. [Perform real-time inference](#D.-Perform-real-time-inference)\n",
    "    5. [Visualize output](#E.-Visualize-output)\n",
    "    6. [Perform streaming inference](#F.-Perform-real-time-inference)\n",
    "3. [Clean-up](#4.-Clean-up)\n",
    "    1. [Delete the model](#A.-Delete-the-model)\n",
    "    2. [Delete the endpoint](#B.-Delete-the-endpoint)\n",
    "    \n",
    "\n",
    "## Usage instructions\n",
    "You can run this notebook one cell at a time (By using Shift+Enter for running a cell).\n",
    "\n",
    "## 1. Select the model package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package_map = {\n",
    "    \"ap-northeast-1\": \"arn:aws:sagemaker:ap-northeast-1:977537786026:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # Tokyo\n",
    "    \"ap-northeast-2\": \"arn:aws:sagemaker:ap-northeast-2:745090734665:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # Seoul\n",
    "    \"ap-south-1\": \"arn:aws:sagemaker:ap-south-1:077584701553:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # Mumbai\n",
    "    \"ap-southeast-1\": \"arn:aws:sagemaker:ap-southeast-1:192199979996:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # Singapore\n",
    "    \"ap-southeast-2\": \"arn:aws:sagemaker:ap-southeast-2:666831318237:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # Sydney\n",
    "    \"ca-central-1\": \"arn:aws:sagemaker:ca-central-1:470592106596:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # Canada Central\n",
    "    \"eu-central-1\": \"arn:aws:sagemaker:eu-central-1:446921602837:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # Frankfurt\n",
    "    \"eu-north-1\": \"arn:aws:sagemaker:eu-north-1:136758871317:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # Stockholm\n",
    "    \"eu-west-1\": \"arn:aws:sagemaker:eu-west-1:985815980388:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # Ireland\n",
    "    \"eu-west-2\": \"arn:aws:sagemaker:eu-west-2:856760150666:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # London\n",
    "    \"eu-west-3\": \"arn:aws:sagemaker:eu-west-3:843114510376:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # Paris\n",
    "    \"sa-east-1\": \"arn:aws:sagemaker:sa-east-1:270155090741:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # São Paulo\n",
    "    \"us-east-1\": \"arn:aws:sagemaker:us-east-1:865070037744:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # N. Virginia\n",
    "    \"us-east-2\": \"arn:aws:sagemaker:us-east-2:057799348421:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # Ohio\n",
    "    \"us-west-1\": \"arn:aws:sagemaker:us-west-1:382657785993:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # N. California\n",
    "    \"us-west-2\": \"arn:aws:sagemaker:us-west-2:594846645681:model-package/supernova-medius-vllm-marketpl-b001f57de24c376c90536618ad543981\",  # Oregon\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip install -qU boto3 sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from IPython.display import Markdown, display\n",
    "from sagemaker import ModelPackage, get_execution_role\n",
    "from sagemaker_streaming import print_event_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in model_package_map.keys():\n",
    "    raise \"UNSUPPORTED REGION\"\n",
    "\n",
    "model_package_arn = model_package_map[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "runtime_sm_client = boto3.client(\"runtime.sagemaker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create an endpoint and perform real-time inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're deploying SuperNova-Medius on a SageMaker real-time endpoint hosted on a GPU instance. If you need general information on real-time inference with Amazon SageMaker, please refer to the SageMaker [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-model.html).\n",
    "\n",
    "The endpoint runs a DJLServing [Deep Learning Container](https://docs.aws.amazon.com/sagemaker/latest/dg/deploy-models-frameworks-djl-serving.html), powered by vLLM. vLLM enables high-performance text generation for the most popular open-source language models. \n",
    "\n",
    "The endpoint configuration focuses on cost efficiency. It uses a [g5.12xlarge](https://aws.amazon.com/ec2/instance-types/g5/) instance. This instance has four NVDIA A10G GPU, with 96 GB of GPU RAM. SuperNova-Medius has 14 billion 16-bit parameters, which can easily fit without the need for quantization.\n",
    "\n",
    "#### OpenAI compatibility\n",
    "\n",
    "The [OpenAI Messages API](https://huggingface.co/docs/text-generation-inference/messages_api) is available. This alllows you to invoke the endpoint in the same way you would invoke an OpenAI model. Likewise, the output format is identical to the OpenAI models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Define the endpoint configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"arcee-ai/SuperNova-Medius\"\n",
    "endpoint_name_prefix = \"SuperNova-Medius\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g5 endpoint\n",
    "\n",
    "real_time_inference_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "model_environment = {\n",
    "    \"OPTION_DTYPE\": \"bf16\",\n",
    "    \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"64\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a deployable model from the model package.\n",
    "model = ModelPackage(\n",
    "    role=role, model_package_arn=model_package_arn, sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# create a unique endpoint name\n",
    "timestamp = \"{:%Y-%m-%d-%H-%M-%S}\".format(datetime.datetime.now())\n",
    "endpoint_name = f\"{model_name}-{timestamp}\"\n",
    "print(f\"Deploying endpoint {endpoint_name}\")\n",
    "\n",
    "# deploy the model\n",
    "response = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=real_time_inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model_data_download_timeout=600,\n",
    "    container_startup_health_check_timeout=600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the endpoint is in service, you will be able to perform real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Define a test payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly and helpful AI assistant.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Suggest 5 names for a new neighborhood pet food store. Names should be short, fun, easy to remember, and respectful of pets. \\\n",
    "        Explain why customers would like them.\",\n",
    "        },\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Perform real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps(model_sample_input),\n",
    ")\n",
    "\n",
    "output = json.loads(response[\"Body\"].read().decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Visualize output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the raw JSON output in OpenAI format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the generated output with Markdown formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(output[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some more examples. Please feel free to tweak them and add your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. Perform streaming inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Please write a friendly marketing pitch for a new SaaS AI platform called Arcee Cloud.\n",
    "We will send this pitch by email to business and technical decision-makers, so make it sound exciting yet professional.\n",
    "The contact email is sales@arcee.ai. Feel free to use emojis as appropriate.\n",
    "Arcee Cloud makes it simple for enterprise users to tailor open-source small language models to their own domain knowledge,\n",
    "in order to build high-quality, cost-effective and secure AI solutions.\"\"\"\n",
    "\n",
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a friendly and helpful Marketing Manager working at Arcee.ai.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(model_sample_input),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "print_event_stream(response['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"As a friendly technical assistant engineer, answer the question in detail.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Why are transformers better models than LSTM?\"},\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(model_sample_input),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "print_event_stream(response['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sample_input = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are Darlene, a friendly and helpful salesperson \\\n",
    "        working at Crystal River Classic Bikes, a classic motorcycle dealership in central Florida.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Using English, write a personalized customer email to get \\\n",
    "        them to sign up for a test ride on the new 2025 motorcycles that are visible at the dealership. \\\n",
    "        Tone should be warm and personal, make sure to weave in the customer information below. \\\n",
    "        Wyatt, your chief mechanic and road captain, has just won the 2024 State Award for Best Mechanic. \\\n",
    "        \\\n",
    "        Customer information:\\\n",
    "        - name: Julien \\\n",
    "        - last visit: 6 months ago for bike service \\\n",
    "        - Owns 2 bikes, a 2002 sporty bike and a 2007 cruiser \\\n",
    "        \",\n",
    "        },\n",
    "    ],\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint_with_response_stream(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(model_sample_input),\n",
    "    ContentType='application/json'\n",
    ")\n",
    "\n",
    "print_event_stream(response['Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have successfully performed a real-time inference, you do not need the endpoint any more. You can terminate the endpoint to avoid being charged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean-up\n",
    "\n",
    "Please don't forget to run the cells below to delete all resources and avoid unecessary charges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(endpoint_name)\n",
    "model.sagemaker_session.delete_endpoint_config(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Delete the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for trying out SuperNova-Medius on SageMaker. We have only scratched the surface of what you can do with this model.\n",
    "\n",
    "We'd be happy to hear from you, learn more about your use case, and help you build your next AI-driven solution. Please reach out to julien@arcee.ai."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
